{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMSCg8b5almbF2/nv2TNORq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":62,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FASgdwNchIx8","executionInfo":{"status":"ok","timestamp":1713926763786,"user_tz":300,"elapsed":819,"user":{"displayName":"Kris Smolarek","userId":"08098082373999393937"}},"outputId":"c3599281-42eb-4216-89e5-ca5b9e56d748"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Mounting the Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","data_root = '/content/drive/My Drive/ChatBot'"]},{"cell_type":"code","source":["import json\n","import string\n","import random\n","\n","import nltk\n","import numpy as np\n","from nltk.stem import WordNetLemmatizer\n","import tensorflow as tf\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","nltk.download(\"punkt\")\n","nltk.download(\"wordnet\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"STpjZtTUiMCH","executionInfo":{"status":"ok","timestamp":1713926763787,"user_tz":300,"elapsed":6,"user":{"displayName":"Kris Smolarek","userId":"08098082373999393937"}},"outputId":"4dac0f81-7fc1-4253-dbed-3c2536c6e185"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":["data_file = open(data_root + '/intents.json').read()\n","data = json.loads(data_file)"],"metadata":{"id":"-jRb4viSi2NF","executionInfo":{"status":"ok","timestamp":1713926763787,"user_tz":300,"elapsed":4,"user":{"displayName":"Kris Smolarek","userId":"08098082373999393937"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["# Creating data_X and data_Y\n","\n","words = []    # for BoW model / vocabulary for patterns\n","classes = []  # for BoW model / vocabulary for tags\n","data_X = []   # for storing each pattern\n","data_y = []   # for storing tag corresponding to each pattern in data_X\n","\n","# Iterating over all the intents\n","\n","for intent in data[\"intents\"]:\n","  for pattern in intent[\"patterns\"]:\n","    tokens = nltk.word_tokenize(pattern) # tokenize each pattern\n","    words.extend(tokens) # and append tokens to words\n","    data_X.append(pattern) # appending pattern to data_X\n","    data_y.append(intent[\"tag\"]) , # appending the associated tag to each pattern\n","\n","    # Adding the tag to classes if it's not already there\n","    if intent[\"tag\"] not in classes:\n","      classes.append(intent[\"tag\"])\n","\n","# Initializing lemmatizer to get stem of words\n","lemmatizer = WordNetLemmatizer()\n","\n","# Lemmatize all the words in the vocaulary and convert them to lowercase\n","# if the words don't appear in punctuation\n","words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n","# Sorting the vocabulary and classes in alphabetical order and taking the #\n","# set to ensure no duplicates occur\n","words = sorted(set(words))\n","classes = sorted(set(classes))"],"metadata":{"id":"hcuf-ZirknQJ","executionInfo":{"status":"ok","timestamp":1713926763787,"user_tz":300,"elapsed":4,"user":{"displayName":"Kris Smolarek","userId":"08098082373999393937"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["# Text to numbers\n","training = []\n","out_empty = [0] * len(classes)\n","# Creating the Bag of Words (BoW) model\n","for idx, doc in enumerate(data_X):\n","  bow = []\n","  text = lemmatizer.lemmatize(doc.lower())\n","  for word in words:\n","    bow.append(1) if word in text else bow.append(0)\n","  # Mark the index of class that the current pattern is associated to\n","  output_row = list(out_empty)\n","  output_row[classes.index(data_y[idx])] = 1\n","  # Add the one hot encoded BoW and associated classes to training\n","  training.append([bow, output_row])\n","# Shuffle the data and convert it to an array\n","random.shuffle(training)\n","training = np.array(training, dtype=object)\n","# Split the features and target labels\n","train_X = np.array(list(training[:, 0]))\n","train_Y = np.array(list(training[:, 1]))"],"metadata":{"id":"DhoaEWIGwjGW","executionInfo":{"status":"ok","timestamp":1713926763913,"user_tz":300,"elapsed":130,"user":{"displayName":"Kris Smolarek","userId":"08098082373999393937"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["# The neural network model\n","model = Sequential()\n","model.add(Dense(128, input_shape=(len(train_X[0]),), activation=\"relu\"))\n","model.add(Dropout(0.5))\n","model.add(Dense(64, activation=\"relu\"))\n","model.add(Dropout(0.5))\n","model.add(Dense(len(train_Y[0]), activation=\"softmax\"))\n","adam = tf.keras.optimizers.legacy.Adam(learning_rate=0.01, decay=1e-6)\n","model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=[\"accuracy\"])\n","print(model.summary())\n","model.fit(x=train_X, y=train_Y, epochs=150, verbose=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i3mYWCND0lHJ","executionInfo":{"status":"ok","timestamp":1713926769796,"user_tz":300,"elapsed":5890,"user":{"displayName":"Kris Smolarek","userId":"08098082373999393937"}},"outputId":"24608e74-f422-4595-b255-6e2fd7c4bd35"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_9\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_27 (Dense)            (None, 128)               7552      \n","                                                                 \n"," dropout_18 (Dropout)        (None, 128)               0         \n","                                                                 \n"," dense_28 (Dense)            (None, 64)                8256      \n","                                                                 \n"," dropout_19 (Dropout)        (None, 64)                0         \n","                                                                 \n"," dense_29 (Dense)            (None, 15)                975       \n","                                                                 \n","=================================================================\n","Total params: 16783 (65.56 KB)\n","Trainable params: 16783 (65.56 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","None\n","Epoch 1/150\n","2/2 [==============================] - 1s 10ms/step - loss: 2.7352 - accuracy: 0.0000e+00\n","Epoch 2/150\n","2/2 [==============================] - 0s 8ms/step - loss: 2.5916 - accuracy: 0.1951\n","Epoch 3/150\n","2/2 [==============================] - 0s 8ms/step - loss: 2.4148 - accuracy: 0.2439\n","Epoch 4/150\n","2/2 [==============================] - 0s 9ms/step - loss: 2.3361 - accuracy: 0.2439\n","Epoch 5/150\n","2/2 [==============================] - 0s 8ms/step - loss: 2.2093 - accuracy: 0.3415\n","Epoch 6/150\n","2/2 [==============================] - 0s 9ms/step - loss: 2.0949 - accuracy: 0.4390\n","Epoch 7/150\n","2/2 [==============================] - 0s 10ms/step - loss: 1.8870 - accuracy: 0.4146\n","Epoch 8/150\n","2/2 [==============================] - 0s 12ms/step - loss: 1.8731 - accuracy: 0.4390\n","Epoch 9/150\n","2/2 [==============================] - 0s 8ms/step - loss: 1.6457 - accuracy: 0.4878\n","Epoch 10/150\n","2/2 [==============================] - 0s 8ms/step - loss: 1.4985 - accuracy: 0.5366\n","Epoch 11/150\n","2/2 [==============================] - 0s 9ms/step - loss: 1.3183 - accuracy: 0.6098\n","Epoch 12/150\n","2/2 [==============================] - 0s 9ms/step - loss: 1.2093 - accuracy: 0.6098\n","Epoch 13/150\n","2/2 [==============================] - 0s 12ms/step - loss: 1.1188 - accuracy: 0.6341\n","Epoch 14/150\n","2/2 [==============================] - 0s 9ms/step - loss: 1.0049 - accuracy: 0.6341\n","Epoch 15/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.8300 - accuracy: 0.8049\n","Epoch 16/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.7549 - accuracy: 0.8293\n","Epoch 17/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.9151 - accuracy: 0.7561\n","Epoch 18/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.7283 - accuracy: 0.7805\n","Epoch 19/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.5577 - accuracy: 0.8293\n","Epoch 20/150\n","2/2 [==============================] - 0s 13ms/step - loss: 0.7714 - accuracy: 0.7317\n","Epoch 21/150\n","2/2 [==============================] - 0s 11ms/step - loss: 0.4201 - accuracy: 0.8780\n","Epoch 22/150\n","2/2 [==============================] - 0s 14ms/step - loss: 0.4464 - accuracy: 0.8293\n","Epoch 23/150\n","2/2 [==============================] - 0s 8ms/step - loss: 0.6832 - accuracy: 0.6829\n","Epoch 24/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.5211 - accuracy: 0.8537\n","Epoch 25/150\n","2/2 [==============================] - 0s 13ms/step - loss: 0.3572 - accuracy: 0.9024\n","Epoch 26/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.4577 - accuracy: 0.8537\n","Epoch 27/150\n","2/2 [==============================] - 0s 8ms/step - loss: 0.3530 - accuracy: 0.9024\n","Epoch 28/150\n","2/2 [==============================] - 0s 8ms/step - loss: 0.4649 - accuracy: 0.8537\n","Epoch 29/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2828 - accuracy: 0.9268\n","Epoch 30/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.2676 - accuracy: 0.9512\n","Epoch 31/150\n","2/2 [==============================] - 0s 11ms/step - loss: 0.2284 - accuracy: 0.9024\n","Epoch 32/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.4046 - accuracy: 0.8537\n","Epoch 33/150\n","2/2 [==============================] - 0s 8ms/step - loss: 0.3521 - accuracy: 0.9024\n","Epoch 34/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.4175 - accuracy: 0.8780\n","Epoch 35/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.2956 - accuracy: 0.9268\n","Epoch 36/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2740 - accuracy: 0.8780\n","Epoch 37/150\n","2/2 [==============================] - 0s 8ms/step - loss: 0.2232 - accuracy: 0.9268\n","Epoch 38/150\n","2/2 [==============================] - 0s 8ms/step - loss: 0.2544 - accuracy: 0.9512\n","Epoch 39/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2438 - accuracy: 0.9756\n","Epoch 40/150\n","2/2 [==============================] - 0s 12ms/step - loss: 0.1293 - accuracy: 0.9756\n","Epoch 41/150\n","2/2 [==============================] - 0s 8ms/step - loss: 0.1931 - accuracy: 0.9756\n","Epoch 42/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2177 - accuracy: 0.9268\n","Epoch 43/150\n","2/2 [==============================] - 0s 8ms/step - loss: 0.3589 - accuracy: 0.8780\n","Epoch 44/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1988 - accuracy: 0.9512\n","Epoch 45/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1506 - accuracy: 0.9756\n","Epoch 46/150\n","2/2 [==============================] - 0s 11ms/step - loss: 0.3702 - accuracy: 0.8780\n","Epoch 47/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1990 - accuracy: 0.9024\n","Epoch 48/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2483 - accuracy: 0.9024\n","Epoch 49/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1214 - accuracy: 0.9512\n","Epoch 50/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2211 - accuracy: 0.9024\n","Epoch 51/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.1864 - accuracy: 0.9512\n","Epoch 52/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1000 - accuracy: 0.9756\n","Epoch 53/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.3827 - accuracy: 0.8780\n","Epoch 54/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2049 - accuracy: 0.9268\n","Epoch 55/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.2058 - accuracy: 0.9268\n","Epoch 56/150\n","2/2 [==============================] - 0s 13ms/step - loss: 0.1899 - accuracy: 0.9024\n","Epoch 57/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.2921 - accuracy: 0.8780\n","Epoch 58/150\n","2/2 [==============================] - 0s 8ms/step - loss: 0.2979 - accuracy: 0.8537\n","Epoch 59/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1720 - accuracy: 0.9512\n","Epoch 60/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1212 - accuracy: 1.0000\n","Epoch 61/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1956 - accuracy: 0.9024\n","Epoch 62/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1468 - accuracy: 0.9756\n","Epoch 63/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.3021 - accuracy: 0.8780\n","Epoch 64/150\n","2/2 [==============================] - 0s 12ms/step - loss: 0.0943 - accuracy: 0.9756\n","Epoch 65/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.1312 - accuracy: 0.9268\n","Epoch 66/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0886 - accuracy: 0.9756\n","Epoch 67/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1089 - accuracy: 0.9756\n","Epoch 68/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0553 - accuracy: 1.0000\n","Epoch 69/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2371 - accuracy: 0.9268\n","Epoch 70/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.1087 - accuracy: 0.9512\n","Epoch 71/150\n","2/2 [==============================] - 0s 14ms/step - loss: 0.0936 - accuracy: 0.9756\n","Epoch 72/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0657 - accuracy: 1.0000\n","Epoch 73/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1088 - accuracy: 0.9756\n","Epoch 74/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.1052 - accuracy: 0.9512\n","Epoch 75/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1309 - accuracy: 0.9512\n","Epoch 76/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1802 - accuracy: 0.9512\n","Epoch 77/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1027 - accuracy: 0.9512\n","Epoch 78/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.1080 - accuracy: 0.9512\n","Epoch 79/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.2681 - accuracy: 0.9024\n","Epoch 80/150\n","2/2 [==============================] - 0s 14ms/step - loss: 0.0510 - accuracy: 1.0000\n","Epoch 81/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.0670 - accuracy: 0.9756\n","Epoch 82/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0621 - accuracy: 0.9756\n","Epoch 83/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1203 - accuracy: 0.9512\n","Epoch 84/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1152 - accuracy: 0.9512\n","Epoch 85/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1387 - accuracy: 0.9512\n","Epoch 86/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1869 - accuracy: 0.9268\n","Epoch 87/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.3128 - accuracy: 0.8780\n","Epoch 88/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.0648 - accuracy: 1.0000\n","Epoch 89/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.1393 - accuracy: 0.9512\n","Epoch 90/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.1116 - accuracy: 0.9512\n","Epoch 91/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0956 - accuracy: 0.9512\n","Epoch 92/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0790 - accuracy: 0.9756\n","Epoch 93/150\n","2/2 [==============================] - 0s 11ms/step - loss: 0.2167 - accuracy: 0.9268\n","Epoch 94/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1228 - accuracy: 0.9512\n","Epoch 95/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2199 - accuracy: 0.9268\n","Epoch 96/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.0837 - accuracy: 0.9756\n","Epoch 97/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0925 - accuracy: 1.0000\n","Epoch 98/150\n","2/2 [==============================] - 0s 12ms/step - loss: 0.1280 - accuracy: 0.9512\n","Epoch 99/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.1048 - accuracy: 0.9512\n","Epoch 100/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1188 - accuracy: 0.9756\n","Epoch 101/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1013 - accuracy: 0.9512\n","Epoch 102/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1331 - accuracy: 0.9512\n","Epoch 103/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1019 - accuracy: 0.9756\n","Epoch 104/150\n","2/2 [==============================] - 0s 12ms/step - loss: 0.1517 - accuracy: 0.9512\n","Epoch 105/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1836 - accuracy: 0.9268\n","Epoch 106/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1984 - accuracy: 0.9512\n","Epoch 107/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0877 - accuracy: 0.9756\n","Epoch 108/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0680 - accuracy: 0.9756\n","Epoch 109/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.0835 - accuracy: 0.9756\n","Epoch 110/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1053 - accuracy: 0.9756\n","Epoch 111/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.0927 - accuracy: 0.9756\n","Epoch 112/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1226 - accuracy: 0.9268\n","Epoch 113/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0664 - accuracy: 0.9512\n","Epoch 114/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.1108 - accuracy: 0.9512\n","Epoch 115/150\n","2/2 [==============================] - 0s 14ms/step - loss: 0.1410 - accuracy: 0.9512\n","Epoch 116/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1508 - accuracy: 0.9756\n","Epoch 117/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1446 - accuracy: 0.9268\n","Epoch 118/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0837 - accuracy: 0.9512\n","Epoch 119/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0362 - accuracy: 1.0000\n","Epoch 120/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1073 - accuracy: 0.9756\n","Epoch 121/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.0813 - accuracy: 0.9756\n","Epoch 122/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0901 - accuracy: 0.9756\n","Epoch 123/150\n","2/2 [==============================] - 0s 14ms/step - loss: 0.0634 - accuracy: 0.9756\n","Epoch 124/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1259 - accuracy: 0.9512\n","Epoch 125/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.0810 - accuracy: 0.9756\n","Epoch 126/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.0932 - accuracy: 0.9756\n","Epoch 127/150\n","2/2 [==============================] - 0s 11ms/step - loss: 0.0421 - accuracy: 0.9756\n","Epoch 128/150\n","2/2 [==============================] - 0s 12ms/step - loss: 0.1381 - accuracy: 0.9268\n","Epoch 129/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1657 - accuracy: 0.9024\n","Epoch 130/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1128 - accuracy: 0.9512\n","Epoch 131/150\n","2/2 [==============================] - 0s 15ms/step - loss: 0.1169 - accuracy: 0.9512\n","Epoch 132/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0770 - accuracy: 0.9756\n","Epoch 133/150\n","2/2 [==============================] - 0s 8ms/step - loss: 0.1769 - accuracy: 0.9512\n","Epoch 134/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0930 - accuracy: 0.9512\n","Epoch 135/150\n","2/2 [==============================] - 0s 11ms/step - loss: 0.1230 - accuracy: 0.9512\n","Epoch 136/150\n","2/2 [==============================] - 0s 13ms/step - loss: 0.0763 - accuracy: 0.9756\n","Epoch 137/150\n","2/2 [==============================] - 0s 11ms/step - loss: 0.2291 - accuracy: 0.9268\n","Epoch 138/150\n","2/2 [==============================] - 0s 12ms/step - loss: 0.0967 - accuracy: 0.9756\n","Epoch 139/150\n","2/2 [==============================] - 0s 11ms/step - loss: 0.0390 - accuracy: 0.9756\n","Epoch 140/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1130 - accuracy: 0.9512\n","Epoch 141/150\n","2/2 [==============================] - 0s 12ms/step - loss: 0.0438 - accuracy: 0.9756\n","Epoch 142/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.1778 - accuracy: 0.9024\n","Epoch 143/150\n","2/2 [==============================] - 0s 11ms/step - loss: 0.0943 - accuracy: 0.9512\n","Epoch 144/150\n","2/2 [==============================] - 0s 12ms/step - loss: 0.2602 - accuracy: 0.9024\n","Epoch 145/150\n","2/2 [==============================] - 0s 11ms/step - loss: 0.1987 - accuracy: 0.9512\n","Epoch 146/150\n","2/2 [==============================] - 0s 11ms/step - loss: 0.2386 - accuracy: 0.9268\n","Epoch 147/150\n","2/2 [==============================] - 0s 11ms/step - loss: 0.0953 - accuracy: 0.9512\n","Epoch 148/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1130 - accuracy: 0.9512\n","Epoch 149/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1518 - accuracy: 0.9268\n","Epoch 150/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.1160 - accuracy: 0.9268\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7e94526639d0>"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["  # Preprocessing the user input (to take strings)\n","def clean_text(text):\n","    tokens = nltk.word_tokenize(text)\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","    return tokens\n","\n","def bag_of_words(text, vocab):\n","    tokens = clean_text(text)\n","    bow = [0] * len(vocab)\n","    for w in tokens:\n","      for idx, word in enumerate(vocab):\n","        if word == w:\n","          bow[idx] = 1\n","    return np.array(bow)\n","\n","def pred_class(text, vocab, labels):\n","    bow = bag_of_words(text, vocab)\n","    result = model.predict(np.array([bow]))[0] # Extracting probabilities\n","    thresh = 0.5\n","    y_pred = [[indx, res] for indx, res in enumerate(result) if res > thresh]\n","    y_pred.sort(key=lambda x: x[1], reverse=True) # Sorting by values of probability in decreasing order\n","    return_list = []\n","    for r in y_pred:\n","      return_list.append(labels[r[0]]) # Contains labels/tags for highest probability\n","    return return_list\n","\n","def get_response(intents_list, intents_json):\n","    if len(intents_list) == 0:\n","      result = \"Sorry! I don't understand.\"\n","    else:\n","      tag = intents_list[0]\n","      list_of_intents = intents_json[\"intents\"]\n","      for i in list_of_intents:\n","        if i[\"tag\"] == tag:\n","          result = random.choice(i[\"responses\"])\n","          break\n","    return result\n","\n","# Interacting with the ChatBot\n","print(\"Press 0 if you don't want to interact with the chatbot.\")\n","while True:\n","    message = input(\"\")\n","    if message == \"0\":\n","      break\n","    intents = pred_class(message, words, classes)\n","    result = get_response(intents, data)\n","    print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":582},"id":"-jmaF5F53vmr","executionInfo":{"status":"error","timestamp":1713927803823,"user_tz":300,"elapsed":926773,"user":{"displayName":"Kris Smolarek","userId":"08098082373999393937"}},"outputId":"1101b739-b203-4091-9987-c9eabe623cda"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["Press 0 if you don't want to interact with the chatbot.\n","hi\n","1/1 [==============================] - 0s 125ms/step\n","Good morning!\n","how are you\n","1/1 [==============================] - 0s 23ms/step\n","Very good and you?\n","great\n","1/1 [==============================] - 0s 22ms/step\n","So, everything's okay!\n","Why wouldn't it be ok\n","1/1 [==============================] - 0s 23ms/step\n","Hi!\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-72-6ad546ceaf72>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Press 0 if you don't want to interact with the chatbot.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"0\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]}]}